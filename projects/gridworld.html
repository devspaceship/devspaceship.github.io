<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Understanding AI</title>
  <meta name="description" content="Personal website">
  <meta name="author" content="Thomas Saint-Gérand">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <link rel="stylesheet" href="/css/custom.css">
  <link rel="stylesheet" href="/prism/prism.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="/images/favicon.png">

  <!-- Script & p5 -->
  <script src="/p5/p5.min.js"></script>
  <script src="/p5/addons/p5.dom.min.js"></script>
  <script src="/scripts/grid.js"></script>
  <script src="/scripts/gridworld.js"></script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
  <script src="/prism/prism.js"></script>

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <!--Navbar-->
    <div style="margin-top: 5%">
      <a class="button button-primary" href="/">Home</a>
      <a class="button button-primary" href="/projects.html">My Projects</a>
      <a class="button button-primary" href="/">Who Am I</a>
    </div>

    <!-- Content -->
    <div style="margin-top: 3%">
      <img class="u-max-full-width" src="/images/student-hat.png" alt="Student Hat" width="128" height="128">
    </div>
    <h1 style="margin-top: 2%"><strong>U</strong>nderstanding <strong>A</strong>rtificial <strong>I</strong>ntelligence</h1>
    
    <!--Content here-->
    <h2>Interactive Gridworld</h2>

    <!--p5 canvas holder-->
    <div id="canvas-holder"></div>

    <label for="block-type">Block Type</label>
    <select id="block-type">
      <option value="trap">Trap</option>
      <option value="start">Starting Point</option>
      <option value="end">End Point</option>
      <option value="air">Air</option>
    </select>

    <h2>What is a gridworld ?</h3>
    <p>Even though the name seems self-explanatory, I am still going to give some precisions.
      This gridworld is 8 lines x 12 columns.
      It represent an <em>environment</em> in which our <em>agent</em> is going to take <em>actions</em>.
      The orange cell on the up left corner is the starting point of our <em>agent</em>.
      The green one placed on the bottom right is the goal <em>state</em>, where we want to go.
      You can drag your mouse on the grid to place traps which are going to be represented by red cells.
      You can erase by placing air again and you can also change the position of the starting and the ending point.
    </p>
    <p>We start at the first time step on the starting point \(s_0\).
      At each time step, we chose a possible action \(a_t\).
      Here we can go down or right but not up or left because there is a wall.
      Let's say that we chosed to go down \(a_0 = down\).
      If there is a trap, we get a 'reward' of \(-100\) and the game ends \(r_0 = -100\).
      If there is nothing, we get a reward of \(-1\), \(r_0 = -1\), and if we got to the end, we get a reward of \(+100\) and the game ends \(r_0 = 100\).
      The goal is to finish the game with the maximum (discounted) reward possible \(R = \sum_{t=0}^{+\infty} \gamma^t r_t\).
      That is why we give a negative reward when transitionning to a cell where there is nothing.
      This tells us that the shortest path to the end is going to be better than making 3 times the tour of the map before going to the end.
    </p>
    <p>The theory lying underground that will help us understand the way we design solving algorithms is the <strong>Reinforcement Learning</strong> theory.
      It is based on a mathematical model that capture the essence of problem like this one called
      <a href="https://en.wikipedia.org/wiki/Markov_decision_process"><strong>Markov Decision Process</strong></a><br>
      In a deterministic <strong>MDP</strong>, you have:
      <ul>
        <li>a set of states \(S\), where \(S_t\) is the subset of states you can access at time step \(t\).</li>
        <li>a set of actions \(A\), where \(A_t\) is the subset of arctions you can chose at time step \(t\).</li>
        <li>a reward function \(R\), where \(R(s, s')\) is the reward you get when transitionning from state \(s\) to \(s'\).</li>
      </ul>
    </p>

    <h2>What is a policy, a state value and a Q-function ?</h2>
    <p>We first define the concept of <em>policy</em>.
      The policy is a function \(\pi : S \rightarrow A\). In other words,
      it takes a state as an input and output an action to take.
      An agent is said to follow a policy \(\pi\) if \( \forall t \in \mathbb{N}, a_t = \pi(s_t)\) i.e. if it takes the action the policy ask it to follow. <br>
      We can now define what are a state value and a Q-value (also called action-value) function given a policy:
      \begin{align}
        V^{\pi}(s_t) &= \mathbb{E}_{\pi} \left[\sum_{i = t}^{+ \infty} \gamma^{i-t} r_i \mid s_t \right] \\
        Q^{\pi}(s_t, a_t) &= \mathbb{E}_{\pi} \left[\sum_{i = t}^{+ \infty} \gamma^{i-t} r_i \mid s_t, a_t \right]
      \end{align}
      We already used \(0 < \gamma \leq 1\) three times without explaining what it was, it is called the <em>discount rate</em>.
      It will set at what extent our agent is going to be concerned by long-time reward.
      If \(\gamma\) is near \(0\), we are going to take into account only reward in the near future.
      If, however, \(\gamma\) is near \(1\), we will considerate more long-lasting reward. <br>
      Ok now, what about \(V^{\pi}\) and \(Q^{\pi}\) ? <br>
      \(V^{\pi}(s_t)\) represents the future discounted reward, starting at \(s_t\) and following the policy \(\pi\). <br>
      \(Q^{\pi}(s_t, a_t)\) is very similar, it is the future discounted reward, still starting at \(s_t\)
      but this time taking action \(a_t\) (possilbly different from \(\pi(s_t)\)) before following the policy.
    </p>
    <p>A policy which is going to maximize the future dicounted reward is called an optimal policy.
      There can be several ones sometimes (for instance if transitioning to nothing would yield a reward of 0 and if the discounting factor was 1). <br>
      Let's take one, we are going to call it \(\pi^*\), we also define the optimal
      state value function \(V^* = V^{\pi^*} = \max_{\pi} V^{\pi}\)
      as well as the optimal Q-value function \(Q^* = Q^{\pi^*} = \max_{\pi} Q^{\pi}\). All optimal policies share the same state-value and action-value (Q-value) function.
    </p>

    <h2>Let's solve it !</h2>
    <p>The next remarks are going to be central for solving the problem,
      they are the <a href="https://en.wikipedia.org/wiki/Bellman_equation">Bellman Equations</a> of a deterministic <em>MDP</em> (with deterministic policy).
      \begin{align}
        V^{\pi}(s_t) &= \sum_{i = t}^{+ \infty} \gamma^{i-t} r_i\\
        &= r_t + \sum_{i = t+1}^{+ \infty} \gamma^{i-t} r_i\\
        &= r_t + \gamma \sum_{i = t+1}^{+ \infty} \gamma^{i-(t+1)} r_i\\
        &= R(s_t, \pi(s_t)) + \gamma V^{\pi}(s_{t+1})
      \end{align}
      By the same reasoning we obtain the equation for the action-value function:
      \begin{align}
        Q^{\pi}(s_t, a_t) &= \sum_{i = t}^{+ \infty} \gamma^{i-t} r_i \mid a_t\\
        &= r_t + \gamma \sum_{i = t+1}^{+ \infty} \gamma^{i-(t+1)} r_i\\
        &= R(s_t, s(s_t, a_t)) + \gamma V^{\pi}(s(s_t, a_t))
      \end{align}
      For the optimal state-value and action-value functions, we have:
      \begin{align}
        V^*(s_t) &= R(s_t, \pi^*(s_t)) + \gamma V^*(s_{t+1})\\
        Q^*(s_t, a_t) &= R(s_t, s(s_t, a_t)) + \gamma V^*(s(s_t, a_t))
      \end{align}
      We can also remark that \(V^*(s) = \max_a Q^*(s, a)\) and inject it in the last equation:
      \[
        Q^*(s_t, a_t) = R(s_t, s(s_t, a_t)) + \gamma \max_{a_{t+1}} Q^*(s(s_t, a_t), a_{t+1})
      \]
    </p>

    <h2>Policy Iteration</h2>
    <p>The first method we are going to use is called Policy Iteration.
        We initialize the state-value function with a random one or
        we can also initialize it at 0 for every state.
        We, then, derive a better policy from this state-value.
        We calculate the new state-value and again a new better policy,
        and so on until the policy is stable.
    </p>
    <h3>Policy Evaluation</h3>
    <p>We first need a method in order to estimate the state-value function of a policy.
        We could solve the \(\mid S \mid\) equations at \(\mid S \mid\) unknowns,
        but we are instead going to use a simpler and less computationally expensive way.
        This technique is called <em>Policy Evaluation</em>.
        For this, we want to use the Bellman equation as an update rule for our state-value estimation:
        \[
          V_{k+1}(s) = R(s, \pi(s)) + \gamma V_k(\pi(s))
        \]
        We can then have two arrays, one for the old values and one for the new values calculated from the old ones.
        We can also do it in-place with one array, replacing values as we go through \(S\).
        Although the latter is usually faster to converge it is anisotropic in the sense that
        the order we are going to do the updates is going to matter.
        In this example, we are going to use the two array version.
        Here, 'N' is the number of rows, 'M' the number of columns and
        'transition' is a function that gives the next state and the reward given the current state and action.
        Finally, treshold is a small value that will determine when we stop the iteration.
    </p>
<pre><code class="language-python">
def policy_evaluation(policy, treshold):
  V_old = [[0]*M for _ in range(N)]
  V_new = [[]*M for _ in range(N)]

  while True:
    delta = 0

    for i in range(N):
      for j in range(M):
        i_, j_, r = transition(i, j, policy(i, j))
        V_new[i][j] = r + gamma*V_old[i_][j_]

        delta = max(delta, abs(V_new[i][j] - V_old[i][j]))

    if delta < treshold:
      break

  return V_new
</code></pre>
    

    <h2>Value Iteration</h2>
    <h2>Monte Carlo Control</h2>
    <h2>SARSA</h2>
    <h2>Q-learning</h2>




    <!-- Licenses -->
    <div style="margin-top: 10%">
      <a class="button button-primary" href="https://github.com/devspaceship">Github</a>
      <a class="button button-primary" href="https://www.linkedin.com/in/devspaceship/">Linked In</a>
      <a class="button button-primary" href="https://www.instagram.com/devspaceship/">Instagram</a>
      <a class="button button-primary" href="https://www.facebook.com/devspaceship">Facebook</a>
    </div>
    Icons made by <a href="https://www.flaticon.com/authors/pixel-perfect" title="Pixel perfect">Pixel perfect</a> 
    from <a href="https://www.flaticon.com/" title="Flaticon">www.flaticon.com</a> 
    is licensed by <a href="http://creativecommons.org/licenses/by/3.0/" title="Creative Commons BY 3.0" target="_blank">CC 3.0 BY</a>
  </div>

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
