<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Understanding AI</title>
  <meta name="description" content="Personal website">
  <meta name="author" content="Thomas Saint-Gérand">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <link rel="stylesheet" href="/css/custom.css">
  <link rel="stylesheet" href="/prism/prism.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="/images/favicon.png">

  <!-- Script & p5 -->
  <script src="/p5/p5.min.js"></script>
  <script src="/p5/addons/p5.dom.min.js"></script>
  <script src="/scripts/gridworld/grid.js"></script>
  <script src="/scripts/gridworld/gridworld.js"></script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
  <script src="/prism/prism.js"></script>

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <!--Navbar-->
    <div style="margin-top: 5%">
      <a class="button button-primary" href="/">Home</a>
      <a class="button button-primary" href="/projects.html">My Projects</a>
    </div>

    <!-- Content -->
    <div style="margin-top: 3%">
      <img class="u-max-full-width" src="/images/student-hat.png" alt="Student Hat" width="128" height="128">
    </div>
    <h1 style="margin-top: 2%"><strong>U</strong>nderstanding <strong>A</strong>rtificial <strong>I</strong>ntelligence</h1>
    
    <!--Content here-->
    <h2>Interactive Gridworld</h2>

    <!--p5 canvas holder-->
    <div id="canvas-holder"></div>

    <label for="block-type">Block Type</label>
    <select id="block-type">
      <option value="trap">Trap</option>
      <option value="start">Starting Point</option>
      <option value="end">End Point</option>
      <option value="air">Air</option>
    </select>

    <label for="solver-type">Solver</label>
    <select id="solver-type">
      <option value="policy_iter">Policy Iteration</option>
      <option value="value_iter">Value Iteration</option>
      <option value="sarsa">SARSA</option>
      <option value="q_learning">Q-Learning</option>
    </select>

    <button class='button-primary' id="solve_button">Solve</button>

    <div id="policy_iter_options" class="row">
      <div class="four columns">
          <label for="policy_iter_treshold">Treshold</label>
          <input type="range" min="-8" max="-2" value="-5" id="policy_iter_treshold">
          <div id="policy_iter_treshold_value"></div>
      </div>
      <div class="four columns">
          <label for="policy_iter_gamma">Gamma</label>
          <input type="range" min="0" max="99" value="97" id="policy_iter_gamma">
          <div id="policy_iter_gamma_value"></div>
      </div>
    </div>

    <div id="value_iter_options" class="row">
      <div class="four columns">
          <label for="value_iter_treshold">Treshold</label>
          <input type="range" min="-8" max="-2" value="-5" id="value_iter_treshold">
          <div id="value_iter_treshold_value"></div>
      </div>
      <div class="four columns">
          <label for="value_iter_gamma">Gamma</label>
          <input type="range" min="0" max="99" value="97" id="value_iter_gamma">
          <div id="value_iter_gamma_value"></div>
      </div>
      <div class="four columns">
        <label for="value_iter_k">Evaluations between improvements</label>
        <input type="range" min="1" max="20" value="7" id="value_iter_k">
        <div id="value_iter_k_value"></div>
      </div>
    </div>

    <div id="SARSA_Q_options">
      <div class="row">
        <div class="four columns">
            <label for="SARSA_Q_N">Number of iterations</label>
            <input type="range" min="4000" max="10000" value="10000" id="SARSA_Q_N">
            <div id="SARSA_Q_N_value"></div>
        </div>
        <div class="four columns">
            <label for="SARSA_Q_gamma">Gamma</label>
            <input type="range" min="0" max="99" value="97" id="SARSA_Q_gamma">
            <div id="SARSA_Q_gamma_value"></div>
        </div>
        <div class="four columns">
          <label for="SARSA_Q_alpha">Learning Rate</label>
          <input type="range" min="1" max="15" value="3" id="SARSA_Q_alpha">
          <div id="SARSA_Q_alpha_value"></div>
        </div>
      </div>
      <div class="row">
        <div class="four columns">
            <label for="SARSA_Q_eps_0">Epsilon 0</label>
            <input type="range" min="0" max="100" value="100" id="SARSA_Q_eps_0">
            <div id="SARSA_Q_eps_0_value"></div>
        </div>
        <div class="four columns">
            <label for="SARSA_Q_T">Exploration Period</label>
            <input type="range" min="1" max="1000" value="350" id="SARSA_Q_T">
            <div id="SARSA_Q_T_value"></div>
        </div>
      </div>
    </div>

    <h2>What is a gridworld ?</h3>
    <p>Even though the name seems self-explanatory, I am still going to give some precisions.
      This gridworld is 8 lines x 12 columns.
      It represent an <em>environment</em> in which our <em>agent</em> is going to take <em>actions</em>.
      The orange cell on the up left corner is the starting point of our <em>agent</em>.
      The green one placed on the bottom right is the goal <em>state</em>, where we want to go.
      You can drag your mouse on the grid to place traps which are going to be represented by red cells.
      You can erase by placing air again and you can also change the position of the starting and the ending point.
    </p>
    <p>We start at the first time step on the starting point \(s_0\).
      At each time step, we chose a possible action \(a_t\).
      Here we can go down or right but not up or left because there is a wall.
      Let's say that we chosed to go down \(a_0 = down\).
      If there is a trap, we get a 'reward' of \(-100\) and the game ends \(r_0 = -100\).
      If there is nothing, we get a reward of \(-1\), \(r_0 = -1\), and if we got to the end, we get a reward of \(+100\) and the game ends \(r_0 = 100\).
      Note that we can define a terminal state as a state where every action takes you to the same state with a reward of \(0\).
      The goal is to finish the game with the maximum (discounted) reward possible \(R = \sum_{t=0}^{+\infty} \gamma^t r_t\).
      That is why we give a negative reward when transitionning to a cell where there is nothing.
      This tells us that the shortest path to the end is going to be better than making 3 times the tour of the map before going to the end.
    </p>
    <p>The theory lying underground that will help us understand the way we design solving algorithms is the <strong>Reinforcement Learning</strong> theory.
      It is based on a mathematical model that capture the essence of problem like this one called
      <a href="https://en.wikipedia.org/wiki/Markov_decision_process"><strong>Markov Decision Process</strong></a><br>
      In a deterministic <strong>MDP</strong>, you have:
      <ul>
        <li>a set of states \(S\), where \(S_t\) is the subset of states you can access at time step \(t\).</li>
        <li>a set of actions \(A\), where \(A_t\) is the subset of arctions you can chose at time step \(t\).</li>
        <li>a reward function \(R\), where \(R(s, s')\) is the reward you get when transitionning from state \(s\) to \(s'\).</li>
      </ul>
    </p>

    <h2>What is a policy, a state value and a Q-function ?</h2>
    <p>We first define the concept of <em>policy</em>.
      The policy is a function \(\pi : S \rightarrow A\). In other words,
      it takes a state as an input and output an action to take.
      An agent is said to follow a policy \(\pi\) if \( \forall t \in \mathbb{N}, a_t = \pi(s_t)\) i.e. if it takes the action the policy ask it to follow. <br>
      We can now define what are a state value and a Q-value (also called action-value) function given a policy:
      \begin{align}
        V^{\pi}(s_t) &= \mathbb{E}_{\pi} \left[\sum_{i = t}^{+ \infty} \gamma^{i-t} r_i \mid s_t \right] \\
        Q^{\pi}(s_t, a_t) &= \mathbb{E}_{\pi} \left[\sum_{i = t}^{+ \infty} \gamma^{i-t} r_i \mid s_t, a_t \right]
      \end{align}
      We already used \(0 < \gamma \leq 1\) three times without explaining what it was, it is called the <em>discount rate</em>.
      It will set at what extent our agent is going to be concerned by long-time reward.
      If \(\gamma\) is near \(0\), we are going to take into account only reward in the near future.
      If, however, \(\gamma\) is near \(1\), we will considerate more long-lasting reward. <br>
      Ok now, what about \(V^{\pi}\) and \(Q^{\pi}\) ? <br>
      \(V^{\pi}(s_t)\) represents the future discounted reward, starting at \(s_t\) and following the policy \(\pi\). <br>
      \(Q^{\pi}(s_t, a_t)\) is very similar, it is the future discounted reward, still starting at \(s_t\)
      but this time taking action \(a_t\) (possilbly different from \(\pi(s_t)\)) before following the policy.
    </p>
    <p>A policy which is going to maximize the future dicounted reward is called an optimal policy.
      There can be several ones sometimes (for instance if transitioning to nothing would yield a reward of 0 and if the discounting factor was 1). <br>
      Let's take one, we are going to call it \(\pi^*\), we also define the optimal
      state value function \(V^* = V^{\pi^*} = \max_{\pi} V^{\pi}\)
      as well as the optimal Q-value function \(Q^* = Q^{\pi^*} = \max_{\pi} Q^{\pi}\). All optimal policies share the same state-value and action-value (Q-value) function.
    </p>

    <h2>Let's solve it !</h2>
    <p>The next remarks are going to be central for solving the problem,
      they are the <a href="https://en.wikipedia.org/wiki/Bellman_equation">Bellman Equations</a> of a deterministic <em>MDP</em> (with deterministic policy).
      \begin{align}
        V^{\pi}(s_t) &= \sum_{i = t}^{+ \infty} \gamma^{i-t} r_i\\
        &= r_t + \sum_{i = t+1}^{+ \infty} \gamma^{i-t} r_i\\
        &= r_t + \gamma \sum_{i = t+1}^{+ \infty} \gamma^{i-(t+1)} r_i\\
        &= R(s_t, \pi(s_t)) + \gamma V^{\pi}(s_{t+1})
      \end{align}
      By the same reasoning we obtain the equation for the action-value function:
      \begin{align}
        Q^{\pi}(s_t, a_t) &= \sum_{i = t}^{+ \infty} \gamma^{i-t} r_i \mid a_t\\
        &= r_t + \gamma \sum_{i = t+1}^{+ \infty} \gamma^{i-(t+1)} r_i\\
        &= R(s_t, s(s_t, a_t)) + \gamma V^{\pi}(s(s_t, a_t))
      \end{align}
      For the optimal state-value and action-value functions, we have:
      \begin{align}
        V^*(s_t) &= R(s_t, \pi^*(s_t)) + \gamma V^*(s_{t+1})\\
        Q^*(s_t, a_t) &= R(s_t, s(s_t, a_t)) + \gamma V^*(s(s_t, a_t))
      \end{align}
      We can also remark that \(V^*(s) = \max_a Q^*(s, a)\) and inject it in the last equation:
      \[
        Q^*(s_t, a_t) = R(s_t, s(s_t, a_t)) + \gamma \max_{a_{t+1}} Q^*(s(s_t, a_t), a_{t+1})
      \]
    </p>

    <h2>Policy Iteration</h2>
    <p>The first method we are going to use is called Policy Iteration.
        We initialize the state-value function with a random one or
        we can also initialize it at 0 for every state.
        We, then, derive a better policy from this state-value.
        We calculate the new state-value and again a new better policy,
        and so on until the policy is stable.
    </p>

    <h3>Policy Evaluation</h3>
    <p>We first need a method in order to estimate the state-value function of a policy.
        We could solve the \(\mid S \mid\) equations at \(\mid S \mid\) unknowns,
        but we are instead going to use a simpler and less computationally expensive way.
        This technique is called <em>Policy Evaluation</em>.
        For this, we want to use the Bellman equation as an update rule for our state-value estimation:
        \[
          V_{k+1}(s) = R(s, \pi(s)) + \gamma V_k(\pi(s))
        \]
        We can then have two arrays, one for the old values and one for the new values calculated from the old ones.
        We can also do it in-place with one array, replacing values as we go through \(S\).
        Although the latter is usually faster to converge it is anisotropic in the sense that
        the order we are going to do the updates is going to matter.
        In this example, we are going to use the two array version.
        Here, 'N' is the number of rows, 'M' the number of columns and
        'transition' is a function that gives the next state and the reward given the current state and action.
        Finally, treshold is a small value that will determine when we stop the iteration.
    </p>
<pre><code class="language-python">def policy_evaluation(policy, treshold, V_old, V_new):
  while True:
    delta = 0

    for i in range(N):
      for j in range(M):
        i_, j_, r = transition(i, j, policy[i][j])
        V_new[i][j] = r + gamma*V_old[i_][j_]

        delta = max(delta, abs(V_new[i][j] - V_old[i][j]))

    if delta < treshold:
      break

    V_old = copy.deepcopy(V_new)
</code></pre>

    <h3>Policy Improvement</h3>
    Once we have evaluated the state-value function,
    we change our policy for a possibly better one:
    \[
      \pi'(s) = \text{argmax}_a r(s, s(s, a)) + \gamma V(s(s, a))
    \]
<pre><code class="language-python">def policy_improvement(V):
  policy = [[]*M for _ in range(N)]

  for i in range(N):
    for j in range(M):
      best = float("-inf")
      best_action = 0

      for action in range(4):
        i_, j_, r = transition(i, j, a)
        if r + gamma*V[i_][j_] > best:
          best = r + gamma*V[i_][j_]
          best_action = action

      policy[i][j] = best_action

  return policy

</code></pre>

    <h3>Final Algorithm</h3>
    We write an helper function to determine if the policy is stable:
<pre><code class="language-python">def is_stable(p1, p2):
  stable = True

  for i in range(N):
    for j in range(M):
      if p1[i][j] != p2[i][j]:
        stable = False

  return stable
</code></pre>
    <p>
      We can now write the full algorithm
    </p>
<pre><code class="language-python">from random import randint

treshold = 1e-5
# i2a = ['UP', 'RIGHT', 'DOWN', 'LEFT']
# a2i = {'UP': 0, 'RIGHT': 1, 'DOWN': 2, 'LEFT': 3}
V_old = [[0]*M for _ in range(N)]
V_new = [[0]*M for _ in range(N)]

pi = [[randint(0, 3) for _ in range(M)] for _ in range(N)]

while True:
  policy_evaluation(pi, treshold, V_old, V_new)
  new_pi = policy_improvement(V_new)

  if is_stable(pi, new_pi):
    break

  pi = new_pi
</code></pre>
    <p>
      We can remark that we use the previous state-value function for evaluating the newt one,
      this have the effect of greatly speeding up the evaluation step.
    </p>

    <h2>Value Iteration</h2>
    <p>
      In Policy Iteration, we were estimating the state-value function of the policies until reasonable convergence.
      In Value Iteration, we instantly greedify the policy between the sweeps for state-value evaluation.
      It discards the need for actually computing the policy between each evaluation.
    </p>
<pre><code class="language-python">def value_iteration(treshold, V_old, V_new):
  while True:
    delta = 0

    for i in range(N):
      for j in range(M):
        best = float("-inf")

        for a in range(4):
          i_, j_, r = transition(i, j, a)

          v = r + gamma*V_old[i_][j_]
          if v > best:
            best = v

        V_new[i][j] = best

        delta = max(delta, abs(V_new[i][j] - V_old[i][j]))

    if delta < treshold:
      break

    V_old = copy.deepcopy(V_new)
</code></pre>
<pre><code class="language-python">from random import randint

treshold = 1e-5
V_old = [[0]*M for _ in range(N)]
V_new = [[0]*M for _ in range(N)]

value_iteration(treshold, V_old, V_new)
pi = policy_improvement(V_new)
</code></pre>
    <p>
      In practice, we usually don't use this algorithm in this form:
      a few steps of classical evaluation are put in between two Value Iteration steps.
      If Value Iteration can be seen as one step of evaluation followed by one policy improvement,
      this version of the algorithm can be seen as \(k\) steps of evaluation followed by an improvement one.
    </p>

    <!-- <h2>Monte Carlo Control</h2>
    <p>
      Monte Carlo Control works in a different way. Instead of evaluating the state-value function from transitions
      from the state space, we evaluate the action-value function and from simulations inside the environment.
      This can be useful when we don't have a model available and we can only sample transitions.
      In order to keep track of our estimates of action-value following a policy,
      we will need to store the episodes transitions to see what was the final return
      of following a policy given an initial state and action.
    </p>
<pre><code class="language-python">from random import randint
# i2a = ['UP', 'RIGHT', 'DOWN', 'LEFT']
# a2i = {'UP': 0, 'RIGHT': 1, 'DOWN': 2, 'LEFT': 3}

Q = [[[0]*4 for _ in range(M)] for _ in range(N)]
pi = [[randint(0, 3) for _ in range(M)] for _ in range(N)]
returns = [[[[]]*4 for _ in range(M)] for _ in range(N)]
</code></pre> -->

    <h2>SARSA</h2>
    <p>
      With SARSA and Q-Learning we are going to be estimating Q instead of V and derive the optimal policy from it.
      Another difference with these methods is that we are going to be simulating episodes
      through the gridworld in order to estimate the Q-function instead of sweeping through the state space.<br>
      SARSA stands for State-Action-Reward-State-Action because we are going to look one action forward following the policy to estimate \(Q(s,a)\).
      In fact we are not always going to follow the policy because it can be biased if it had not tried certain actions.
      To ensure that we explore the state-action space we need to take a random action once in a while.
      Since we don't need to explore as much as in the beginning episodes after episodes we can decrease the probability over time.
      This probability of chosing a random action in a given state is represented by \(\epsilon\)
      which decreases in \(\frac{1}{t}\) where \(t\) represent the epsiode.
    </p>
<pre><code class='language-python'>def choose_action(pi, i, j, eps_0, T, t):
  eps = eps_0/(1+(t/T))

  if random() < eps:
    a = randint(0, 3)
  else:
    a = pi[i][j]

  return a
</code></pre>
    <p>
      Here we use \(\epsilon = \frac{\epsilon_0}{1+t/T}\). \(\epsilon_0\) is \(\epsilon\) at time-step \(t=0\) and \(T\)
      represents the decreasing period. Choosing a random action with probability \(\epsilon\) and the greedy action otherwise
      is called an \(\epsilon\)-greedy policy.
    </p>
<pre><code class='language-python'>Q = [[[0]*4 for _ in range(M)] for _ in range(N)]
pi = [[randint(0, 3) for _ in range(M)] for _ in range(N)]

for t in range(num_iter):
  i = randint(0, N - 1)
  j = randint(0, M - 1)
  a = choose_action(pi, i, j, eps_0, T, t)

  while True:
    i_, j_, r = transition(i, j, a)
    a_ = choose_action(pi, i_, j_, eps_0, T, t)

    Q[i][j][a] = (1 - alpha)*Q[i][j][a] + alpha*(r + gamma*Q[i_][j_][a_])
    
    i, j, a = i_, j_, a_
    if is_terminal(i, j):
      break

  pi = Q_to_policy(Q)
</code></pre>
    <p>
      Remark that we do not completely update \(Q(s,a)\) to \(r + \gamma*Q(s', a')\),
      we make a step in that direction of proportion \(\alpha\), this parameter is the 
      learning rate of our algorithm.
    </p>

    <h2>Q-Learning</h2>
    Q-Learning is basically the same as SARSA, it only differs in its update rule:
    instead of choosing a second action following the \(\epsilon\)-greedy policy,
    we choose the best action available in state \(s'\):
    \(\max_{a'} Q(s', a')\) which is, in fact, the state-value of \(s'\): \(V(s')\) for the greedy policy.
<pre><code class='language-python'>Q = [[[0]*4 for _ in range(M)] for _ in range(N)]
pi = [[randint(0, 3) for _ in range(M)] for _ in range(N)]

for t in range(num_iter):
  i = randint(0, N - 1)
  j = randint(0, M - 1)
  a = choose_action(pi, i, j, eps_0, T, t)

  while True:
    i_, j_, r = transition(i, j, a)
    a_ = choose_action(pi, i_, j_, eps_0, T, t)

    Q[i][j][a] = (1 - alpha)*Q[i][j][a] + alpha*(r + gamma*max(old_Q[i_][j_]))
    
    i, j, a = i_, j_, a_
    if is_terminal(i, j):
      break

  pi = Q_to_policy(Q)
</code></pre>

    <h2>Final Note</h2>
    <p>
      The code presented here is halfway between Python and pseudo-code.
      If you want to see the actual code used here (JavaScript - p5),
      <a href="https://github.com/devspaceship/devspaceship.github.io/tree/master/scripts/gridworld">look here</a>.
      Also, a lot of simplifications have been done in order to explain more directly
      the different concepts. Finally, we only looked at the case of deterministic MDP with known model.
      To have a better view of Reinforcement Learning, I recommend this excellent book about it
      written by <a href="http://incompleteideas.net/book/bookdraft2017nov5.pdf">Sutton & Barto</a>
    </p>

    <h2>Still To Do</h2>
    <ul>
      <li>Create gridworld - <em>Done</em></li>
      <li>Finsih algorithm explainations - <em>Done</em></li>
      <li>Create DOM menu - <em>Done</em></li>
      <li>Display values next to sliders - <em>Done</em></li>
      <li>Implement the algorithms - <em>Done</em></li>
      <li>Tune the sliders</li>
      <li>Add explaining images</li>
    </ul>

    <!-- Licenses -->
    <div style="margin-top: 10%">
      <a class="button button-primary" href="https://github.com/devspaceship">Github</a>
      <a class="button button-primary" href="https://www.linkedin.com/in/devspaceship/">Linked In</a>
      <a class="button button-primary" href="https://www.instagram.com/devspaceship/">Instagram</a>
      <a class="button button-primary" href="https://www.facebook.com/devspaceship">Facebook</a>
    </div>
    Icons made by <a href="https://www.flaticon.com/authors/pixel-perfect" title="Pixel perfect">Pixel perfect</a> 
    from <a href="https://www.flaticon.com/" title="Flaticon">www.flaticon.com</a> 
    is licensed by <a href="http://creativecommons.org/licenses/by/3.0/" title="Creative Commons BY 3.0" target="_blank">CC 3.0 BY</a>
  </div>

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
