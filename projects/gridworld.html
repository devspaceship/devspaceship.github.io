<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Understanding AI</title>
  <meta name="description" content="Personal website">
  <meta name="author" content="Thomas Saint-Gérand">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <link rel="stylesheet" href="/css/custom.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="/images/favicon.png">

  <!-- Script & p5 -->
  <script src="/p5/p5.min.js"></script>
  <script src="/p5/addons/p5.dom.min.js"></script>
  <script src="/scripts/grid.js"></script>
  <script src="/scripts/gridworld.js"></script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <!--Navbar-->
    <div style="margin-top: 5%">
      <a class="button button-primary" href="/">Home</a>
      <a class="button button-primary" href="/projects.html">My Projects</a>
      <a class="button button-primary" href="/">Who Am I</a>
    </div>

    <!-- Content -->
    <div style="margin-top: 3%">
      <img class="u-max-full-width" src="/images/student-hat.png" alt="Student Hat" width="128" height="128">
    </div>
    <h1 style="margin-top: 2%"><strong>U</strong>nderstanding <strong>A</strong>rtificial <strong>I</strong>ntelligence</h1>
    
    <!--Content here-->
    <h2>Interactive Gridworld</h2>

    <!--p5 canvas holder-->
    <div id="canvas-holder"></div>

    <label for="block-type">Block Type</label>
    <select id="block-type">
      <option value="trap">Trap</option>
      <option value="start">Starting Point</option>
      <option value="end">End Point</option>
      <option value="air">Air</option>
    </select>

    <h2>What is a gridworld ?</h3>
    <p>Even though the name seems self-explanatory, I am still going to give some precisions.
      This gridworld is 8 lines x 12 columns.
      It represent an <em>environment</em> in which our <em>agent</em> is going to take <em>actions</em>.
      The orange cell on the up left corner is the starting point of our <em>agent</em>.
      The green one placed on the bottom right is the goal <em>state</em>, where we want to go.
      You can drag your mouse on the grid to place traps which are going to be represented by red cells.
      You can erase by placing air again and you can also change the position of the starting and the ending point.
    </p>
    <p>We start at the first time step on the starting point \(s_0\).
      At each time step, we chose a possible action \(a_t\).
      Here we can go down or right but not up or left because there is a wall.
      Let's say that we chosed to go down \(a_0 = down\).
      If there is a trap, we get a 'reward' of \(-100\) and the game ends \(r_0 = -100\).
      If there is nothing, we get a reward of \(-1\), \(r_0 = -1\), and if we got to the end, we get a reward of \(+100\) and the game ends \(r_0 = 100\).
      The goal is to finish the game with the maximum (discounted) reward possible \(R = \sum_{t=0}^{+\infty} \gamma^t r_t\).
      That is why we give a negative reward when transitionning to a cell where there is nothing.
      This tells us that the shortest path to the end is going to be better than making 3 times the tour of the map before going to the end.
    </p>
    <p>The theory lying underground that will help us understand the way we design solving algorithms is the <strong>Reinforcement Learning</strong> theory.
      It is based on a mathematical model that capture the essence of problem like this one called
      <a href="https://en.wikipedia.org/wiki/Markov_decision_process"><strong>Markov Decision Process</strong></a><br>
      In a deterministic <strong>MDP</strong>, you have:
      <ul>
        <li>a set of states \(S\), where \(S_t\) is the subset of states you can access at time step \(t\).</li>
        <li>a set of actions \(A\), where \(A_t\) is the subset of arctions you can chose at time step \(t\).</li>
        <li>a reward function \(R\), where \(R(s, s')\) is the reward you get when transitionning from state \(s\) to \(s'\).</li>
      </ul>
    </p>

    <h2>What is a policy, a state value and a Q-function ?</h2>
    <p>We first define the concept of <em>policy</em>.
      The policy is a function \(\pi : S \rightarrow A\). In other words,
      it takes a state as an input and output an action to take.
      An agent is said to follow a policy \(\pi\) if \( \forall t \in \mathbb{N}, a_t = \pi(s_t)\) i.e. if it takes the action the policy ask it to follow. <br>
      We can now define what are a state value and a Q-function given a policy:
      \begin{align}
        V^{\pi}(s_t) &= \mathbb{E}_{\pi} \left[\sum_{i = t}^{+ \infty} \gamma^{i-t} r_i \mid s_t \right] \\
        Q^{\pi}(s_t, a_t) &= \mathbb{E}_{\pi} \left[\sum_{i = t}^{+ \infty} \gamma^{i-t} r_i \mid s_t, a_t \right]
      \end{align}
      We already used \(0 < \gamma \leq 1\) three times without explaining what it was, it is called the <em>discount rate</em>.
      It will set at what extent our agent is going to be concerned by long-time reward.
      If \(\gamma\) is near \(0\), we are going to take into account only reward in the near future.
      If, however, \(\gamma\) is near \(1\), we will considerate more long-lasting reward.


      What are those.
    </p>

    <h3>To Do :</h3>
    <ul>
      <li>Create DOM elements to tune the learning algorithms</li>
      <li>Implement Value Iteration, Monte-Carlo, SARSA, Q-learning, etc.</li>
      <li>Explain everything</li>
    </ul>

    <!-- Licenses -->
    <div style="margin-top: 10%">
      <a class="button button-primary" href="https://github.com/devspaceship">Github</a>
      <a class="button button-primary" href="https://www.linkedin.com/in/devspaceship/">Linked In</a>
      <a class="button button-primary" href="https://www.instagram.com/devspaceship/">Instagram</a>
      <a class="button button-primary" href="https://www.facebook.com/devspaceship">Facebook</a>
    </div>
    Icons made by <a href="https://www.flaticon.com/authors/pixel-perfect" title="Pixel perfect">Pixel perfect</a> 
    from <a href="https://www.flaticon.com/" title="Flaticon">www.flaticon.com</a> 
    is licensed by <a href="http://creativecommons.org/licenses/by/3.0/" title="Creative Commons BY 3.0" target="_blank">CC 3.0 BY</a>
  </div>

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
